{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from keras import layers, models, optimizers\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/james/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(999999)\n",
    "train = pd.read_csv('raw_data/fulltrain.csv', header = None, names=['class','text'])\n",
    "test = pd.read_csv(\"raw_data/balancedtest.csv\", header = None, names=['class','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "3    17870\n",
       "1    14047\n",
       "4     9995\n",
       "2     6942\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = train.loc[train['class'] == 1]\n",
    "two = train.loc[train['class'] == 2]\n",
    "three = train.loc[train['class'] == 3]\n",
    "four = train.loc[train['class'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, one.sample(n=3953), two.sample(n=6000), two.sample(n=5058), three.sample(n=130), four.sample(n=8005)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    18000\n",
       "2    18000\n",
       "3    18000\n",
       "4    18000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['text']\n",
    "x_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=700)\n",
    "tokenizer.fit_on_texts(x_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Padding\n",
    "padding_type = 'post'\n",
    "truncating_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "max_length = 100\n",
    "\n",
    "X_train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=truncating_type)\n",
    "X_test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=truncating_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1                          \n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(train_sequences, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(test_sequences, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def create_embedding_matrix(model_path, word_index, embedding_dim):\n",
    "    word_vectors = KeyedVectors.load(model_path)   \n",
    "    vocab_size = len(word_index) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    # Iterate through word_index to create the embedding matrix\n",
    "    for word, idx in word_index.items():\n",
    "        if word in word_vectors.key_to_index:  # Check if the word is in the KeyedVectors\n",
    "            embedding_vector = word_vectors[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx] = embedding_vector[:embedding_dim]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Assuming your tokenizer has been fitted on your text data\n",
    "embedding_dim = 300  # Dimensionality of Google News Word2Vec vectors\n",
    "model_path = 'word2vec-google-news-300.model' \n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(model_path, tokenizer.word_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['class'] - 1\n",
    "y_test = test['class'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embedding_dim = 300\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    Conv1D(128, 7, kernel_regularizer=tf.keras.regularizers.l2( l2 = 0.3), activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer to reduce overfitting\n",
    "    Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),'accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 119ms/step - accuracy: 0.5398 - loss: 4.8340 - precision: 0.6683 - recall: 0.2848 - val_accuracy: 0.6881 - val_loss: 0.9249 - val_precision: 0.7007 - val_recall: 0.6525\n",
      "Epoch 2/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 120ms/step - accuracy: 0.7916 - loss: 0.8267 - precision: 0.8265 - recall: 0.7383 - val_accuracy: 0.7099 - val_loss: 0.9018 - val_precision: 0.7284 - val_recall: 0.6778\n",
      "Epoch 3/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 121ms/step - accuracy: 0.8187 - loss: 0.8017 - precision: 0.8459 - recall: 0.7827 - val_accuracy: 0.6895 - val_loss: 1.0250 - val_precision: 0.7016 - val_recall: 0.6721\n",
      "Epoch 4/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 129ms/step - accuracy: 0.8236 - loss: 0.7825 - precision: 0.8493 - recall: 0.7924 - val_accuracy: 0.7734 - val_loss: 0.8210 - val_precision: 0.7850 - val_recall: 0.7581\n",
      "Epoch 5/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 124ms/step - accuracy: 0.8348 - loss: 0.7726 - precision: 0.8553 - recall: 0.8058 - val_accuracy: 0.7943 - val_loss: 0.8529 - val_precision: 0.8089 - val_recall: 0.7772\n",
      "Epoch 6/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 121ms/step - accuracy: 0.8405 - loss: 0.7666 - precision: 0.8609 - recall: 0.8148 - val_accuracy: 0.8171 - val_loss: 0.7849 - val_precision: 0.8305 - val_recall: 0.8004\n",
      "Epoch 7/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 121ms/step - accuracy: 0.8412 - loss: 0.7583 - precision: 0.8604 - recall: 0.8169 - val_accuracy: 0.8326 - val_loss: 0.7104 - val_precision: 0.8443 - val_recall: 0.8210\n",
      "Epoch 8/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 122ms/step - accuracy: 0.8451 - loss: 0.7583 - precision: 0.8644 - recall: 0.8229 - val_accuracy: 0.7283 - val_loss: 0.9525 - val_precision: 0.7416 - val_recall: 0.7140\n",
      "Epoch 9/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 121ms/step - accuracy: 0.8480 - loss: 0.7636 - precision: 0.8664 - recall: 0.8273 - val_accuracy: 0.7878 - val_loss: 0.8248 - val_precision: 0.7944 - val_recall: 0.7801\n",
      "Epoch 10/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 122ms/step - accuracy: 0.8497 - loss: 0.7538 - precision: 0.8672 - recall: 0.8296 - val_accuracy: 0.7806 - val_loss: 0.8262 - val_precision: 0.7929 - val_recall: 0.7675\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x383168550>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train_categorical, \n",
    "          validation_split=0.2,  # Use a portion of the training data for validation\n",
    "          epochs=20, batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    Conv1D(128, 7, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2=0.3)),\n",
    "    MaxPooling1D(pool_size=2),  # Optional pooling layer after the first convolution\n",
    "    Conv1D(256, 5, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2=0.3)),  # Additional convolutional layer\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer to reduce overfitting\n",
    "    Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),'accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 130ms/step - accuracy: 0.3127 - loss: 9.1991 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.0090 - val_loss: 1.6458 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 129ms/step - accuracy: 0.3075 - loss: 1.3574 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 1.6533 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 129ms/step - accuracy: 0.3153 - loss: 1.3550 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.0090 - val_loss: 1.6740 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 130ms/step - accuracy: 0.3119 - loss: 1.3555 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 1.6640 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x383ccf550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train_padded, y_train_categorical, \n",
    "          validation_split=0.2,  # Use a portion of the training data for validation\n",
    "          epochs=20, batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = model1.predict(X_test_padded)\n",
    "y_pred_classes1 = y_pred1.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on validation = 0.455602919317178\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(y_test, y_pred_classes, average='macro')\n",
    "print('Score on validation = {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on validation = 0.1\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(y_test, y_pred_classes1, average='macro')\n",
    "print('Score on validation = {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.21      0.27       750\n",
      "           1       0.55      0.51      0.53       750\n",
      "           2       0.35      0.51      0.42       750\n",
      "           3       0.58      0.64      0.61       750\n",
      "\n",
      "    accuracy                           0.47      3000\n",
      "   macro avg       0.47      0.47      0.46      3000\n",
      "weighted avg       0.47      0.47      0.46      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report on test data:')\n",
    "# Make sure y_test is in the correct format for comparison\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report on test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       750\n",
      "           1       0.00      0.00      0.00       750\n",
      "           2       0.25      1.00      0.40       750\n",
      "           3       0.00      0.00      0.00       750\n",
      "\n",
      "    accuracy                           0.25      3000\n",
      "   macro avg       0.06      0.25      0.10      3000\n",
      "weighted avg       0.06      0.25      0.10      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/james/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/james/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Classification report on test data:')\n",
    "# Make sure y_test is in the correct format for comparison\n",
    "print(classification_report(y_test, y_pred_classes1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
