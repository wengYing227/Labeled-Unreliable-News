{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyI5SA6HIoRH"
   },
   "source": [
    "# CS4248 Project\n",
    "(DON'T CLICK RUN ALL! Lemmatization takes tooo long and pls don't run it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T12:43:08.052298Z",
     "start_time": "2024-03-03T12:43:07.925947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS033yHSHyAx",
    "outputId": "e923c9e4-6bda-4a5a-970a-3bdd5ee92c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
      "Collecting numpy==1.25.1\n",
      "  Downloading numpy-1.25.1-cp310-cp310-win_amd64.whl (15.0 MB)\n",
      "     ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/15.0 MB 5.0 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/15.0 MB 5.8 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 1.0/15.0 MB 7.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 1.0/15.0 MB 5.8 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.5/15.0 MB 6.4 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.8/15.0 MB 6.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.2/15.0 MB 6.7 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.6/15.0 MB 7.1 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.0/15.0 MB 7.1 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.4/15.0 MB 7.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.9/15.0 MB 7.5 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.3/15.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.7/15.0 MB 7.7 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 5.2/15.0 MB 7.9 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.6/15.0 MB 8.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.9/15.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.4/15.0 MB 8.1 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.8/15.0 MB 8.0 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.2/15.0 MB 8.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 7.6/15.0 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.3/15.0 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.8/15.0 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.1/15.0 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.6/15.0 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.2/15.0 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.4/15.0 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 10.8/15.0 MB 9.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 11.0/15.0 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.6/15.0 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 11.9/15.0 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 12.1/15.0 MB 8.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 12.5/15.0 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 12.9/15.0 MB 9.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.4/15.0 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 13.7/15.0 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 14.1/15.0 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 14.4/15.0 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/15.0 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.0/15.0 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.0/15.0 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting pandas==2.1.1\n",
      "  Downloading pandas-2.1.1-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "     ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/10.7 MB 33.5 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.8/10.7 MB 8.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.3/10.7 MB 10.6 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 1.7/10.7 MB 9.1 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 2.2/10.7 MB 10.1 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 2.4/10.7 MB 8.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.7/10.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.9/10.7 MB 7.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.3/10.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.3/10.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 3.5/10.7 MB 6.9 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.0/10.7 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.3/10.7 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.7/10.7 MB 7.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.2/10.7 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.6/10.7 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.9/10.7 MB 7.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.4/10.7 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.7/10.7 MB 7.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.1/10.7 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.5/10.7 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.1/10.7 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.5/10.7 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 8.8/10.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.4/10.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.9/10.7 MB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.2/10.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.7 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.7/10.7 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyspellchecker==0.8.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 5)) (1.4.1.post1)\n",
      "Collecting tqdm==4.65.0\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: textblob in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 7)) (0.18.0.post0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.1->-r requirements.txt (line 3)) (1.16.0)\n",
      "Installing collected packages: tqdm, numpy, pandas\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.0\n",
      "    Uninstalling pandas-2.0.0:\n",
      "      Successfully uninstalled pandas-2.0.0\n",
      "Successfully installed numpy-1.25.1 pandas-2.1.1 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# %cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "# !cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:33.330247Z",
     "start_time": "2024-03-05T16:49:33.275009Z"
    },
    "id": "2nqSeqYQJ4vW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.929285Z",
     "start_time": "2024-03-05T16:49:34.039945Z"
    },
    "id": "9PepPt_CL94x"
   },
   "outputs": [],
   "source": [
    "csv.field_size_limit(999999)\n",
    "train = pd.read_csv('raw_data/fulltrain.csv', header = None, names=['class','text'])\n",
    "X_train = train['text']\n",
    "y_train = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.935720Z",
     "start_time": "2024-03-05T16:49:34.929618Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sCT1xI_yXFJO",
    "outputId": "172bcde2-37a1-40c2-e329-f0152f49ab50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:35.839149Z",
     "start_time": "2024-03-05T16:49:35.784315Z"
    },
    "id": "-FcMBozxWAvz"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"raw_data/balancedtest.csv\", header = None, names=['class','text'])\n",
    "X_test = test['text']\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:36.665109Z",
     "start_time": "2024-03-05T16:49:36.648989Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s5befmSMXN8N",
    "outputId": "c1aef16d-ccb9-4036-f8af-1a6c9d16c035"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  When so many actors seem content to churn out ...\n",
       "1      1   In what football insiders are calling an unex...\n",
       "2      1  In a freak accident following Game 3 of the N....\n",
       "3      1  North Koreas official news agency announced to...\n",
       "4      1  The former Alaska Governor Sarah Palin would b..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WOxbEhspgUT"
   },
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:47.137182Z",
     "start_time": "2024-03-05T16:49:37.686948Z"
    },
    "id": "NKfM8O-nbbpJ"
   },
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,1), max_features = 10000)\n",
    "\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(X_train_tfidf, y_train, X_test_tfidf, y_test):\n",
    "\n",
    "  nb_classifier = ComplementNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "  y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "  return y_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621308Z",
     "start_time": "2024-03-05T16:49:53.568257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQyWjGLUxREy",
    "outputId": "e4827d40-70fe-44b0-ecd2-9c271deaf5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.61      0.61       750\n",
      "           2       0.62      0.46      0.53       750\n",
      "           3       0.62      0.93      0.74       750\n",
      "           4       0.88      0.67      0.76       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.68      0.67      0.66      3000\n",
      "weighted avg       0.68      0.67      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "# NB - baseline\n",
    "y_pred_nb = NB(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITNfwpWkt1OI"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6Yrnd2f_y5W"
   },
   "source": [
    "### Lemmatization with POS Tagging\n",
    "(DON'T RUN AGAIN! It takes tooooo long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621497Z",
     "start_time": "2024-03-05T16:49:53.605689Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E3t6cQp1r-N",
    "outputId": "45aad408-e2fc-4eea-d48b-c4cd161199bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/stella/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_with_pos(tokens):\n",
    "  # tokenize the sentence and find the POS tag for each token\n",
    "  pos_tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "  # print(pos_tagged)\n",
    "\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "  # print(wordnet_tagged)\n",
    "\n",
    "  lemmatized_sentence = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "      if tag is None:\n",
    "          # if there is no available tag, append the token as is\n",
    "          lemmatized_sentence.append(word)\n",
    "      else:\n",
    "          # else use the tag to lemmatize the token\n",
    "          lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "  # lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "  # print(lemmatized_sentence)\n",
    "  return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:00:54.576560Z",
     "start_time": "2024-03-05T16:49:53.615691Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-3cE9Qzt3zY",
    "outputId": "dd07285c-1ccd-41b5-c9d3-78d8174a7cb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/stella/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/stella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 48854/48854 [10:27<00:00, 77.91it/s] \n",
      "100%|██████████| 3000/3000 [00:33<00:00, 88.88it/s] \n"
     ]
    }
   ],
   "source": [
    "# Start preprocessing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def preprocessing(data):\n",
    "  data_clean = []\n",
    "  for sentence in tqdm(data):\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Remove punctuation and number\n",
    "    tokens = [w for w in tokens if (not w in punc) and (not w.isdigit())]\n",
    "\n",
    "    # Spell check\n",
    "    # Taking too long time for each sentence, not practical to be used\n",
    "    # tokens = [spell.correction(w) for w in tokens]\n",
    "    # tokens = [w for w in tokens if w is not None and len(w) > 0]\n",
    "\n",
    "    # Lemmatization based on tagging\n",
    "    tokens = lemmatize_with_pos(tokens)\n",
    "\n",
    "    data_clean.append((' ').join(tokens))\n",
    "  return data_clean\n",
    "\n",
    "X_train_clean = preprocessing(X_train)\n",
    "X_test_clean = preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:47.789779Z",
     "start_time": "2024-03-05T17:04:45.983859Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(X_train_clean, X_test_clean):\n",
    "    with open('raw_data/X_train_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_train_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "    with open('raw_data/X_test_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_test_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "# Uncomment if you want to save X_train_clean and X_test_clean\n",
    "# write_to_csv(X_train_clean, X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:53.998277Z",
     "start_time": "2024-03-05T17:04:53.082771Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_from_file():\n",
    "    a_df = pd.read_csv('raw_data/X_train_clean.csv', header=None, dtype=str)\n",
    "    a = a_df[0].values\n",
    "    \n",
    "    b_df = pd.read_csv('raw_data/X_test_clean.csv', header=None, dtype=str)\n",
    "    b = b_df[0].values\n",
    "    return a, b\n",
    "\n",
    "# Uncomment if you want to read from file\n",
    "a, b = read_from_file()\n",
    "X_train_clean, X_test_clean = a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:05:12.655783Z",
     "start_time": "2024-03-05T17:05:12.605867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu2641T-HNd3",
    "outputId": "ba6f10d5-8b1f-4365-be7e-11ca479109bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.65      0.51      0.57       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n"
     ]
    }
   ],
   "source": [
    "X_train_clean_tfidf, X_test_clean_tfidf = tfidf(X_train_clean, X_test_clean)\n",
    "\n",
    "_ = NB(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords, duplicates, and contradicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:48.406043Z",
     "start_time": "2024-03-05T17:05:14.905065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48652/48652 [01:26<00:00, 564.82it/s] \n",
      "100%|██████████| 2990/2990 [00:04<00:00, 606.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text): #trivial casefolding\n",
    "    return text.lower()\n",
    "\n",
    "#Obtain set of stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    data_removed_stopwords = []\n",
    "    for sentence in tqdm(data):\n",
    "        #tokenize sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        #remove word if found in set of stopwords\n",
    "        words_wo_stopwords = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        data_removed_stopwords.append(' '.join(words_wo_stopwords))\n",
    "    return data_removed_stopwords\n",
    "\n",
    "def remove_dupes(data):\n",
    "    #use pandas to drop duplicates, only removes those with equal values in both class and text columns\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "def remove_contradictions(data):\n",
    "    #group by text and filter groups with more than one unique label(contradicting labels)\n",
    "    contradictions = data.groupby('text').filter(lambda x: x['class'].nunique() > 1)\n",
    "    \n",
    "    #remove all texts with contradicting labels since we do not know which is the correct label\n",
    "    return data[~data['text'].isin(contradictions['text'])]\n",
    "\n",
    "def preprocess2(data):\n",
    "    processed_data = remove_dupes(data)\n",
    "    processed_data = remove_contradictions(processed_data)\n",
    "    processed_data['text'] = remove_stopwords(processed_data['text'])\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "train_clean2 = preprocess2(train)    \n",
    "test_clean2 = preprocess2(test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:54.749892Z",
     "start_time": "2024-03-05T17:06:48.424600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_clean2 = train_clean2['text']\n",
    "y_train_clean2 = train_clean2['class']\n",
    "X_test_clean2 = test_clean2['text']\n",
    "y_test_clean2 = test_clean2['class']\n",
    "\n",
    "X_train_clean2_tfidf, X_test_clean2_tfidf = tfidf(X_train_clean2, X_test_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:58.286818Z",
     "start_time": "2024-03-05T17:06:58.232950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.60      0.60       745\n",
      "           2       0.61      0.45      0.52       750\n",
      "           3       0.61      0.92      0.73       747\n",
      "           4       0.87      0.67      0.76       748\n",
      "\n",
      "    accuracy                           0.66      2990\n",
      "   macro avg       0.67      0.66      0.65      2990\n",
      "weighted avg       0.67      0.66      0.65      2990\n"
     ]
    }
   ],
   "source": [
    "_ = NB(X_train_clean2_tfidf, y_train_clean2, X_test_clean2_tfidf, y_test_clean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTufYR0Gt4Wi"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:08:08.491871Z",
     "start_time": "2024-03-05T17:06:58.330465Z"
    },
    "id": "h4_ImWMZt6Ru"
   },
   "outputs": [],
   "source": [
    "# Text Subjectivity (w/o preprocessing)\n",
    "from textblob import TextBlob\n",
    "\n",
    "def subjectivity(data, tf_idf):\n",
    "    # new_data = data.copy()\n",
    "    new_data = data\n",
    "    new_data['psycho-linguistic'] = new_data[\"text\"].apply(lambda x: TextBlob(x).sentiment)\n",
    "    new_data['subjectivity'] = new_data['psycho-linguistic'].apply(lambda x: x[1])\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    subject_df = pd.merge(tfidf_df, new_data['subjectivity'],left_index=True, right_index=True)\n",
    "    subject_df.columns = subject_df.columns.astype(str)\n",
    "    return subject_df\n",
    "\n",
    "train_subject_df = subjectivity(train, X_train_tfidf)\n",
    "test_subject_df = subjectivity(test, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:09:13.136870Z",
     "start_time": "2024-03-05T17:09:11.435035Z"
    },
    "id": "qLCfOSineuZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.62      0.62       750\n",
      "           2       0.62      0.46      0.53       750\n",
      "           3       0.62      0.93      0.74       750\n",
      "           4       0.88      0.67      0.76       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.68      0.67      0.66      3000\n",
      "weighted avg       0.68      0.67      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_subject_df, y_train, test_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:10:22.321745Z",
     "start_time": "2024-03-05T17:09:13.137714Z"
    },
    "id": "Z8wmIXUvewcx"
   },
   "outputs": [],
   "source": [
    "# Text Subjectivity (with preprocessing)\n",
    "train_clean_subject_df = subjectivity(train, X_train_clean_tfidf)\n",
    "test_clean_subject_df = subjectivity(test, X_test_clean_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:11:16.446900Z",
     "start_time": "2024-03-05T17:11:14.310628Z"
    },
    "id": "3Uyp0kzUe1V_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.64      0.51      0.57       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.87      0.65      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_clean_subject_df, y_train, test_clean_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "We will adjust the possible parameters, i.e. n_grams, max_features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram: 1, max_feature: 10000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.81      0.83       750\n",
      "           2       0.84      0.42      0.56       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.92      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.75      0.74      3000\n",
      "weighted avg       0.77      0.74      0.74      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.65      0.51      0.57       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n",
      "\n",
      "n_gram: 1, max_feature: 20000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.82      0.83       750\n",
      "           2       0.85      0.42      0.56       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.75      3000\n",
      "   macro avg       0.78      0.75      0.74      3000\n",
      "weighted avg       0.78      0.75      0.74      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.61      0.61       750\n",
      "           2       0.63      0.48      0.55       750\n",
      "           3       0.62      0.92      0.74       750\n",
      "           4       0.88      0.65      0.75       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.66      3000\n",
      "weighted avg       0.69      0.67      0.66      3000\n",
      "\n",
      "n_gram: 1, max_feature: None\n",
      "Number of features: 220626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.81      0.83       750\n",
      "           2       0.84      0.41      0.56       750\n",
      "           3       0.59      0.84      0.70       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.74      0.73      3000\n",
      "weighted avg       0.77      0.74      0.73      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.53      0.59       750\n",
      "           2       0.60      0.12      0.21       750\n",
      "           3       0.39      0.99      0.56       750\n",
      "           4       0.95      0.44      0.60       750\n",
      "\n",
      "    accuracy                           0.52      3000\n",
      "   macro avg       0.65      0.52      0.49      3000\n",
      "weighted avg       0.65      0.52      0.49      3000\n",
      "\n",
      "n_gram: 2, max_feature: 10000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.75      0.80       750\n",
      "           2       0.83      0.35      0.49       750\n",
      "           3       0.55      0.83      0.66       750\n",
      "           4       0.76      0.92      0.83       750\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.75      0.71      0.69      3000\n",
      "weighted avg       0.75      0.71      0.69      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.61      0.61       750\n",
      "           2       0.64      0.46      0.54       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.82      0.68      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.68      0.67      0.66      3000\n",
      "weighted avg       0.68      0.67      0.66      3000\n",
      "\n",
      "n_gram: 2, max_feature: 20000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.78      0.81       750\n",
      "           2       0.84      0.34      0.48       750\n",
      "           3       0.56      0.83      0.67       750\n",
      "           4       0.77      0.92      0.84       750\n",
      "\n",
      "    accuracy                           0.72      3000\n",
      "   macro avg       0.76      0.72      0.70      3000\n",
      "weighted avg       0.76      0.72      0.70      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.64      0.63       750\n",
      "           2       0.64      0.47      0.54       750\n",
      "           3       0.64      0.91      0.75       750\n",
      "           4       0.84      0.69      0.76       750\n",
      "\n",
      "    accuracy                           0.68      3000\n",
      "   macro avg       0.69      0.68      0.67      3000\n",
      "weighted avg       0.69      0.68      0.67      3000\n",
      "\n",
      "n_gram: 2, max_feature: None\n",
      "Number of features: 4774152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.81      0.82       750\n",
      "           2       0.85      0.34      0.48       750\n",
      "           3       0.58      0.87      0.70       750\n",
      "           4       0.80      0.90      0.85       750\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.77      0.73      0.71      3000\n",
      "weighted avg       0.77      0.73      0.71      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.12      0.20       750\n",
      "           2       0.77      0.01      0.03       750\n",
      "           3       0.28      1.00      0.43       750\n",
      "           4       1.00      0.20      0.33       750\n",
      "\n",
      "    accuracy                           0.33      3000\n",
      "   macro avg       0.69      0.33      0.25      3000\n",
      "weighted avg       0.69      0.33      0.25      3000\n",
      "\n",
      "n_gram: 3, max_feature: 10000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.75      0.80       750\n",
      "           2       0.82      0.33      0.47       750\n",
      "           3       0.55      0.83      0.66       750\n",
      "           4       0.75      0.92      0.83       750\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.75      0.71      0.69      3000\n",
      "weighted avg       0.75      0.71      0.69      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.63      0.62       750\n",
      "           2       0.64      0.42      0.51       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.81      0.68      0.74       750\n",
      "\n",
      "    accuracy                           0.66      3000\n",
      "   macro avg       0.67      0.66      0.65      3000\n",
      "weighted avg       0.67      0.66      0.65      3000\n",
      "\n",
      "n_gram: 3, max_feature: 20000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.77      0.81       750\n",
      "           2       0.84      0.32      0.46       750\n",
      "           3       0.55      0.82      0.66       750\n",
      "           4       0.76      0.92      0.83       750\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.75      0.71      0.69      3000\n",
      "weighted avg       0.75      0.71      0.69      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.66      0.63       750\n",
      "           2       0.66      0.45      0.53       750\n",
      "           3       0.66      0.89      0.76       750\n",
      "           4       0.82      0.71      0.76       750\n",
      "\n",
      "    accuracy                           0.68      3000\n",
      "   macro avg       0.68      0.68      0.67      3000\n",
      "weighted avg       0.68      0.68      0.67      3000\n",
      "\n",
      "n_gram: 3, max_feature: None\n",
      "Number of features: 18739817\n"
     ]
    }
   ],
   "source": [
    "# All this need Kfold cross validation in the future\n",
    "\n",
    "n_grams = [1, 2, 3, 4]\n",
    "max_features = [10000, 20000, None]\n",
    "def tfidf(X_train, X_test, n_gram, max_feature):\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,n_gram), max_features = max_feature)\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "for n_gram in n_grams:\n",
    "  for max_feature in max_features:\n",
    "    X_train_tfidf, X_test_tfidf = tfidf(X_train_clean, X_test_clean, n_gram, max_feature)\n",
    "    print(f\"n_gram: {n_gram}, max_feature: {max_feature}\")\n",
    "    num_features = X_train_tfidf.shape[1]\n",
    "    if max_feature is None:\n",
    "      print(f\"Number of features: {num_features}\")\n",
    "    _ = NB(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "\n",
    "# Ran until n_gram = 3 for 30min ><"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "#### GloVe\n",
    "- Download from https://nlp.stanford.edu/projects/glove/\n",
    "- Used Wikipedia 2014 + Gigaword 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    glove_model = {}\n",
    "    with open(glove_file_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            # embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            try:\n",
    "                coefs = np.asarray(split_line[1:], dtype='float32')\n",
    "            except ValueError:\n",
    "                pass\n",
    "            glove_model[word] = coefs\n",
    "    return glove_model\n",
    "\n",
    "glove_model = load_glove_model(\"glove.6B/glove.6B.300d.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "X_train_processed = X_train.apply(preprocess_text)\n",
    "X_test_processed = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc, model):\n",
    "    # Filter out words that are not in the embedding\n",
    "    embeddings = [model[word] for word in doc if word in model]\n",
    "    if not embeddings:\n",
    "        # If no words in the document are in the model, return a vector of zeros\n",
    "        return np.zeros(next(iter(model.values())).shape)\n",
    "    # Average the embeddings\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "X_train_vectors = np.array([document_vector(doc, glove_model) for doc in X_train_processed])\n",
    "X_test_vectors = np.array([document_vector(doc, glove_model) for doc in X_test_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cleaned data + GloVe embeddings + Naive Bayes\n",
    "# _ = NB(glove_embedding_features, y_train, glove_embedding_features_test, y_test)\n",
    "# ValueError: Negative values in data passed to ComplementNB (input X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:28:24.733933Z",
     "start_time": "2024-03-05T17:27:58.208190Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hedging Word Count\n",
    "# Reference: all 176 hedging words in hedges.txt are from https://github.com/words/hedges\n",
    "import re\n",
    "\n",
    "def fetch_hedging_regex(filename='hedges.txt'):\n",
    "    hedges = []\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    for l in lines:\n",
    "        hedges.append(l.strip())\n",
    "    hedges_re = \"(?:^|\\W)(\" + \"|\".join(hedges) + \")(?:$|\\W)\"\n",
    "    return hedges_re\n",
    "\n",
    "def hedges(x, hedges_re):\n",
    "    res = re.findall(hedges_re, x.lower())\n",
    "    counts = sum(1 for s in res if s != '')\n",
    "    if counts > 0:\n",
    "        return 1 \n",
    "    return 0\n",
    "\n",
    "def hedging_word_count(data, tf_idf, hedge_re):\n",
    "    new_data = data.copy()\n",
    "    new_data[\"hedges\"] = new_data.loc[:, \"text\"].apply(hedges, hedges_re=hedge_re)\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    hedged_df = pd.merge(tfidf_df, new_data['hedges'],left_index=True, right_index=True)\n",
    "    hedged_df.columns = hedged_df.columns.astype(str)\n",
    "    return hedged_df\n",
    "\n",
    "hedges_re = fetch_hedging_regex()\n",
    "train_hedge_df = hedging_word_count(train, X_train_tfidf, hedges_re)\n",
    "test_hedge_df = hedging_word_count(test, X_test_tfidf, hedges_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:29:15.025973Z",
     "start_time": "2024-03-05T17:29:12.842389Z"
    },
    "id": "V_EZcIXQfD-t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.62      0.62       750\n",
      "           2       0.62      0.46      0.53       750\n",
      "           3       0.62      0.93      0.74       750\n",
      "           4       0.88      0.67      0.76       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.66      3000\n",
      "weighted avg       0.69      0.67      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_hedge_df, y_train, test_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:29:40.772374Z",
     "start_time": "2024-03-05T17:29:15.066556Z"
    },
    "id": "rYz1yM-ffFyo"
   },
   "outputs": [],
   "source": [
    "# Hedging Word Count (with preprocessing)\n",
    "train_clean_hedge_df = hedging_word_count(train, X_train_clean_tfidf, hedges_re)\n",
    "test_clean_hedge_df = hedging_word_count(test, X_test_clean_tfidf, hedges_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:30:48.552354Z",
     "start_time": "2024-03-05T17:30:46.675254Z"
    },
    "id": "GzySa4unfL_F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.64      0.51      0.57       750\n",
      "           3       0.63      0.91      0.75       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_clean_hedge_df, y_train, test_clean_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/james/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T13:38:37.122924Z",
     "start_time": "2024-03-03T13:38:37.121971Z"
    },
    "id": "HgSb6hNwfNmO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_features(data, tf_idf):\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    new_data['sentiments'] = new_data['text'].apply(lambda x: sia.polarity_scores(x))\n",
    "    new_data['compound'] = new_data['sentiments'].apply(lambda x: x['compound'])\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    sentiment_df = pd.merge(tfidf_df, new_data['compound'], left_index=True, right_index=True)\n",
    "    sentiment_df.columns = sentiment_df.columns.astype(str)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "train_sentiment_df = sentiment_features(train, X_train_tfidf)\n",
    "test_sentiment_df = sentiment_features(test, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.64      0.62       750\n",
      "           2       0.62      0.45      0.52       750\n",
      "           3       0.62      0.92      0.74       750\n",
      "           4       0.88      0.66      0.75       750\n",
      "\n",
      "    accuracy                           0.66      3000\n",
      "   macro avg       0.68      0.66      0.66      3000\n",
      "weighted avg       0.68      0.66      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Scores may be negative so NB will not work. For NB, I have added a constant to the sentiment scores to make them all positive.\n",
    "min_compound_train = train_sentiment_df['compound'].min()\n",
    "min_compound_test = test_sentiment_df['compound'].min()\n",
    "min_compound = min(min_compound_train, min_compound_test)\n",
    "\n",
    "if min_compound < 0:\n",
    "    adjustment = abs(min_compound)\n",
    "    train_sentiment_df['compound'] += adjustment\n",
    "    test_sentiment_df['compound'] += adjustment\n",
    "\n",
    "_ = NB(train_sentiment_df, y_train, test_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Preprocessing\n",
    "train_clean_sentiment_df = sentiment_features(train, X_train_clean_tfidf)\n",
    "test_clean_sentiment_df = sentiment_features(test, X_test_clean_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.62      0.61       750\n",
      "           2       0.63      0.48      0.54       750\n",
      "           3       0.62      0.91      0.74       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.66      3000\n",
      "   macro avg       0.68      0.66      0.66      3000\n",
      "weighted avg       0.68      0.66      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Scores may be negative so NB will not work. For NB, I have added a constant to the sentiment scores to make them all positive.\n",
    "min_clean_compound_train = train_clean_sentiment_df['compound'].min()\n",
    "min_clean_compound_test = test_clean_sentiment_df['compound'].min()\n",
    "min_clean_compound = min(min_clean_compound_train, min_clean_compound_test)\n",
    "\n",
    "if min_clean_compound < 0:\n",
    "    adjustment = abs(min_clean_compound)\n",
    "    train_clean_sentiment_df['compound'] += adjustment\n",
    "    test_clean_sentiment_df['compound'] += adjustment\n",
    "    \n",
    "_ = NB(train_clean_sentiment_df, y_train, test_clean_sentiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Length\n",
    "- Number of quotations\n",
    "- Count of word types:\n",
    "    - Numbers\n",
    "    - Proper nouns\n",
    "    - Conjunctions\n",
    "    - Superlatives\n",
    "    - 1st / 3rd pronoun counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def add_custom_feature_to_tfidf(data, tfidf_matrix, feature_extraction_func):\n",
    "    custom_feature = data.apply(feature_extraction_func)\n",
    "    custom_feature_sparse = np.array(custom_feature).reshape(-1, 1)\n",
    "    enhanced_tfidf_matrix = hstack([tfidf_matrix, custom_feature_sparse])\n",
    "    return enhanced_tfidf_matrix\n",
    "\n",
    "# Feature extraction functions\n",
    "def count_sentence_length(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_length_df = add_custom_feature_to_tfidf(X_train, X_train_tfidf, count_sentence_length)\n",
    "test_length_df = add_custom_feature_to_tfidf(X_test, X_test_tfidf, count_sentence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.86      0.74       750\n",
      "           2       0.72      0.42      0.53       750\n",
      "           3       0.56      0.93      0.70       750\n",
      "           4       0.90      0.39      0.55       750\n",
      "\n",
      "    accuracy                           0.65      3000\n",
      "   macro avg       0.71      0.65      0.63      3000\n",
      "weighted avg       0.71      0.65      0.63      3000\n"
     ]
    }
   ],
   "source": [
    "# For some reason, after scaling sentence length can give negative values so I will just use NB with unscaled sentence length.\n",
    "\n",
    "_ = NB(train_length_df, y_train, test_length_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
