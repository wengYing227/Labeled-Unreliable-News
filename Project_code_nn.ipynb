{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyI5SA6HIoRH"
   },
   "source": [
    "# CS4248 Project\n",
    "(DON'T CLICK RUN ALL! Lemmatization takes tooo long and pls don't run it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T12:43:08.052298Z",
     "start_time": "2024-03-03T12:43:07.925947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS033yHSHyAx",
    "outputId": "e923c9e4-6bda-4a5a-970a-3bdd5ee92c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: numpy==1.25.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.25.1)\n",
      "Requirement already satisfied: pandas==2.1.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: pyspellchecker==0.8.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.4.1.post1)\n",
      "Requirement already satisfied: tqdm==4.65.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: textblob in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.18.0.post0)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.10/site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.1.1->-r requirements.txt (line 3)) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# %cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "# !cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:33.330247Z",
     "start_time": "2024-03-05T16:49:33.275009Z"
    },
    "id": "2nqSeqYQJ4vW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "/var/folders/j2/q86pqgyn7bz6wlqds1ml4h_40000gn/T/ipykernel_38104/117655612.py:27: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_filename='glove.6B.100d.txt'\n",
    "# If you are running for the first time, uncomment the below 2 lines\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "glove_path = os.path.abspath(os.path.join('.', glove_filename))\n",
    "\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "\n",
    "glove2word2vec(glove_path, word2vec_output_file)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.929285Z",
     "start_time": "2024-03-05T16:49:34.039945Z"
    },
    "id": "9PepPt_CL94x"
   },
   "outputs": [],
   "source": [
    "csv.field_size_limit(999999)\n",
    "train = pd.read_csv('raw_data/fulltrain.csv', header = None, names=['class','text'])\n",
    "X_train = train['text']\n",
    "y_train = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.935720Z",
     "start_time": "2024-03-05T16:49:34.929618Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sCT1xI_yXFJO",
    "outputId": "172bcde2-37a1-40c2-e329-f0152f49ab50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:35.839149Z",
     "start_time": "2024-03-05T16:49:35.784315Z"
    },
    "id": "-FcMBozxWAvz"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"raw_data/balancedtest.csv\", header = None, names=['class','text'])\n",
    "X_test = test['text']\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:36.665109Z",
     "start_time": "2024-03-05T16:49:36.648989Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s5befmSMXN8N",
    "outputId": "c1aef16d-ccb9-4036-f8af-1a6c9d16c035"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  When so many actors seem content to churn out ...\n",
       "1      1   In what football insiders are calling an unex...\n",
       "2      1  In a freak accident following Game 3 of the N....\n",
       "3      1  North Koreas official news agency announced to...\n",
       "4      1  The former Alaska Governor Sarah Palin would b..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WOxbEhspgUT"
   },
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:47.137182Z",
     "start_time": "2024-03-05T16:49:37.686948Z"
    },
    "id": "NKfM8O-nbbpJ"
   },
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,1), max_features = 10000)\n",
    "\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.567610Z",
     "start_time": "2024-03-05T16:49:47.138293Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDCcR3tOpJ-e",
    "outputId": "c1bc4bd9-bf63-4b1a-902b-147b1c243360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:  2024-03-27 21:19:17.799890\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.79      0.82       750\n",
      "           2       0.82      0.38      0.52       750\n",
      "           3       0.57      0.83      0.67       750\n",
      "           4       0.80      0.92      0.86       750\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.76      0.73      0.72      3000\n",
      "weighted avg       0.76      0.73      0.72      3000\n",
      "\n",
      "Training finish:  2024-03-27 21:19:28.288851\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - baseline\n",
    "\n",
    "def LR(X_train_tfidf, y_train, X_test_tfidf, y_test):\n",
    "  print(\"Training start: \", datetime.datetime.now())\n",
    "\n",
    "  LR_classifier = LogisticRegression(random_state = 42, max_iter=2000).fit(X_train_tfidf, y_train)\n",
    "\n",
    "  y_pred_lr = LR_classifier.predict(X_test_tfidf)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_lr))\n",
    "  print(\"Training finish: \", datetime.datetime.now())\n",
    "\n",
    "  return y_pred_lr\n",
    "\n",
    "y_pred_lr = LR(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621308Z",
     "start_time": "2024-03-05T16:49:53.568257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQyWjGLUxREy",
    "outputId": "e4827d40-70fe-44b0-ecd2-9c271deaf5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.61      0.62       750\n",
      "           2       0.62      0.46      0.53       750\n",
      "           3       0.62      0.93      0.74       750\n",
      "           4       0.88      0.67      0.76       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.68      0.67      0.66      3000\n",
      "weighted avg       0.68      0.67      0.66      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB - baseline\n",
    "\n",
    "def NB(X_train_tfidf, y_train, X_test_tfidf, y_test):\n",
    "\n",
    "  nb_classifier = ComplementNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "  y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "  return y_pred_nb\n",
    "\n",
    "y_pred_nb = NB(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITNfwpWkt1OI"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6Yrnd2f_y5W"
   },
   "source": [
    "### Lemmatization with POS Tagging\n",
    "(DON'T RUN AGAIN! It takes tooooo long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621497Z",
     "start_time": "2024-03-05T16:49:53.605689Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E3t6cQp1r-N",
    "outputId": "45aad408-e2fc-4eea-d48b-c4cd161199bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_with_pos(tokens):\n",
    "  # tokenize the sentence and find the POS tag for each token\n",
    "  pos_tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "  # print(pos_tagged)\n",
    "\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "  # print(wordnet_tagged)\n",
    "tqmd\n",
    "  lemmatized_sentence = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "      if tag is None:\n",
    "          # if there is no available tag, append the token as is\n",
    "          lemmatized_sentence.append(word)\n",
    "      else:\n",
    "          # else use the tag to lemmatize the token\n",
    "          lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "  # lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "  # print(lemmatized_sentence)\n",
    "  return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:00:54.576560Z",
     "start_time": "2024-03-05T16:49:53.615691Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-3cE9Qzt3zY",
    "outputId": "dd07285c-1ccd-41b5-c9d3-78d8174a7cb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "# Start preprocessing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def preprocessing(data):\n",
    "  data_clean = []\n",
    "  for sentence in tqdm(data):\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Remove punctuation and number\n",
    "    tokens = [w for w in tokens if (not w in punc) and (not w.isdigit())]\n",
    "\n",
    "    # Spell check\n",
    "    # Taking too long time for each sentence, not practical to be used\n",
    "    # tokens = [spell.correction(w) for w in tokens]\n",
    "    # tokens = [w for w in tokens if w is not None and len(w) > 0]\n",
    "\n",
    "    # Lemmatization based on tagging\n",
    "    tokens = lemmatize_with_pos(tokens)\n",
    "\n",
    "    data_clean.append((' ').join(tokens))\n",
    "  return data_clean\n",
    "\n",
    "X_train_clean = preprocessing(X_train)\n",
    "X_test_clean = preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:47.789779Z",
     "start_time": "2024-03-05T17:04:45.983859Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(X_train_clean, X_test_clean):\n",
    "    with open('raw_data/X_train_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_train_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "    with open('raw_data/X_test_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_test_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "# Uncomment if you want to save X_train_clean and X_test_clean\n",
    "# write_to_csv(X_train_clean, X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:53.998277Z",
     "start_time": "2024-03-05T17:04:53.082771Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_from_file():\n",
    "    a_df = pd.read_csv('raw_data/X_train_clean.csv', header=None, dtype=str)\n",
    "    a = a_df[0].values\n",
    "    \n",
    "    b_df = pd.read_csv('raw_data/X_test_clean.csv', header=None, dtype=str)\n",
    "    b = b_df[0].values\n",
    "    return a, b\n",
    "\n",
    "# Uncomment if you want to read from file\n",
    "a, b = read_from_file()\n",
    "X_train_clean, X_test_clean = a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "y_train_labeled = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_test_labeled = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:05:11.041172Z",
     "start_time": "2024-03-05T17:04:55.869549Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH-JuFWdzqHa",
    "outputId": "1e641367-5188-4e56-8a83-b04dbde46ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:  2024-03-27 21:19:50.334498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.81      0.83       750\n",
      "           2       0.84      0.42      0.56       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.74      0.73      3000\n",
      "weighted avg       0.77      0.74      0.73      3000\n",
      "\n",
      "Training finish:  2024-03-27 21:19:58.324094\n"
     ]
    }
   ],
   "source": [
    "X_train_clean_tfidf, X_test_clean_tfidf = tfidf(X_train_clean, X_test_clean)\n",
    "\n",
    "_ = LR(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:05:12.655783Z",
     "start_time": "2024-03-05T17:05:12.605867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu2641T-HNd3",
    "outputId": "ba6f10d5-8b1f-4365-be7e-11ca479109bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.65      0.51      0.57       750\n",
      "           3       0.63      0.92      0.75       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,1), max_features = 10000)\n",
    "\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "X_train_clean_tfidf, X_test_clean_tfidf = tfidf(X_train_clean, X_test_clean)\n",
    "\n",
    "# _ = LR(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('queen')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D*3))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in tqdm(data):\n",
    "      tokens = nltk.word_tokenize(sentence.lower())\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = np.concatenate((vecs.mean(axis=0), vecs.min(axis=0), vecs.max(axis=0)))\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Number of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 48854/48854 [03:28<00:00, 233.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found: 2 / 48854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer(word2vec_model)\n",
    "X_train_word2vec = word2vec_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3000/3000 [00:12<00:00, 247.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with no words found: 0 / 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply to test\n",
    "X_test_word2vec = word2vec_vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48854, 10000)\n",
      "(48854, 300)\n",
      "(48854, 10300)\n"
     ]
    }
   ],
   "source": [
    "X_train_combined = np.concatenate((X_train_clean_tfidf.toarray(), X_train_word2vec), axis=1)\n",
    "# X_train_combined =  X_train_word2vec\n",
    "print(X_train_clean_tfidf.shape)\n",
    "print(X_train_word2vec.shape)\n",
    "print(X_train_combined.shape)\n",
    "\n",
    "X_test_combined = np.concatenate((X_test_clean_tfidf.toarray(), X_test_word2vec), axis=1)\n",
    "# X_test_combined =  X_test_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:  2024-03-21 10:05:32.712848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.79      0.82       750\n",
      "           2       0.71      0.37      0.48       750\n",
      "           3       0.53      0.70      0.60       750\n",
      "           4       0.75      0.92      0.82       750\n",
      "\n",
      "    accuracy                           0.69      3000\n",
      "   macro avg       0.71      0.69      0.68      3000\n",
      "weighted avg       0.71      0.69      0.68      3000\n",
      "\n",
      "Training finish:  2024-03-21 12:40:47.537759\n"
     ]
    }
   ],
   "source": [
    "_ = LR(X_train_combined, y_train, X_test_combined, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_196 (Dense)           (None, 256)               2637056   \n",
      "                                                                 \n",
      " dropout_114 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2670468 (10.19 MB)\n",
      "Trainable params: 2670468 (10.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1527/1527 [==============================] - 41s 26ms/step - loss: 0.6486 - accuracy: 0.7406 - f1_score: 0.7158 - val_loss: 0.7724 - val_accuracy: 0.7163 - val_f1_score: 0.7090\n",
      "Epoch 2/5\n",
      "1527/1527 [==============================] - 42s 28ms/step - loss: 0.2192 - accuracy: 0.9232 - f1_score: 0.9175 - val_loss: 0.6895 - val_accuracy: 0.7637 - val_f1_score: 0.7632\n",
      "Epoch 3/5\n",
      "1527/1527 [==============================] - 44s 29ms/step - loss: 0.1445 - accuracy: 0.9503 - f1_score: 0.9468 - val_loss: 0.7727 - val_accuracy: 0.7543 - val_f1_score: 0.7500\n",
      "Epoch 4/5\n",
      "1527/1527 [==============================] - 43s 28ms/step - loss: 0.1112 - accuracy: 0.9616 - f1_score: 0.9592 - val_loss: 1.3386 - val_accuracy: 0.6470 - val_f1_score: 0.6304\n",
      "Epoch 5/5\n",
      "1527/1527 [==============================] - 40s 26ms/step - loss: 0.0934 - accuracy: 0.9685 - f1_score: 0.9665 - val_loss: 0.8396 - val_accuracy: 0.7600 - val_f1_score: 0.7554\n",
      "94/94 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.81      0.81       750\n",
      "           2       0.75      0.52      0.62       750\n",
      "           3       0.64      0.81      0.71       750\n",
      "           4       0.87      0.90      0.88       750\n",
      "\n",
      "    accuracy                           0.76      3000\n",
      "   macro avg       0.77      0.76      0.76      3000\n",
      "weighted avg       0.77      0.76      0.76      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam,Adagrad\n",
    "import tensorflow_addons as tfa\n",
    "from keras import regularizers\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='sigmoid', input_shape=(X_train_combined.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                metrics=['accuracy', tfa.metrics.F1Score(average='macro',num_classes=4)])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(X_train_combined, y_train_labeled, epochs=epochs, validation_data=(X_test_combined, y_test_labeled))\n",
    "\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "y_pred_inverse = one_hot_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred_inverse))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
