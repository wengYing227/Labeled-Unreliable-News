{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyI5SA6HIoRH"
   },
   "source": [
    "# CS4248 Project\n",
    "(DON'T CLICK RUN ALL! Lemmatization takes tooo long and pls don't run it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T12:43:08.052298Z",
     "start_time": "2024-03-03T12:43:07.925947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS033yHSHyAx",
    "outputId": "e923c9e4-6bda-4a5a-970a-3bdd5ee92c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: numpy==1.25.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.25.1)\n",
      "Requirement already satisfied: pandas==2.1.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: pyspellchecker==0.8.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.4.1.post1)\n",
      "Requirement already satisfied: tqdm==4.65.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: textblob in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.18.0.post0)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.10/site-packages (from pandas==2.1.1->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.10/site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn==1.4.1.post1->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.1.1->-r requirements.txt (line 3)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# %cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "# !cd \"/content/drive/My Drive/CS4248 Project\"\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:33.330247Z",
     "start_time": "2024-03-05T16:49:33.275009Z"
    },
    "id": "2nqSeqYQJ4vW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/james/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.929285Z",
     "start_time": "2024-03-05T16:49:34.039945Z"
    },
    "id": "9PepPt_CL94x"
   },
   "outputs": [],
   "source": [
    "csv.field_size_limit(999999)\n",
    "train = pd.read_csv('raw_data/fulltrain.csv', header = None, names=['class','text'])\n",
    "X_train = train['text']\n",
    "y_train = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:34.935720Z",
     "start_time": "2024-03-05T16:49:34.929618Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sCT1xI_yXFJO",
    "outputId": "172bcde2-37a1-40c2-e329-f0152f49ab50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:35.839149Z",
     "start_time": "2024-03-05T16:49:35.784315Z"
    },
    "id": "-FcMBozxWAvz"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"raw_data/balancedtest.csv\", header = None, names=['class','text'])\n",
    "X_test = test['text']\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:36.665109Z",
     "start_time": "2024-03-05T16:49:36.648989Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s5befmSMXN8N",
    "outputId": "c1aef16d-ccb9-4036-f8af-1a6c9d16c035"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      1  When so many actors seem content to churn out ...\n",
       "1      1   In what football insiders are calling an unex...\n",
       "2      1  In a freak accident following Game 3 of the N....\n",
       "3      1  North Koreas official news agency announced to...\n",
       "4      1  The former Alaska Governor Sarah Palin would b..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WOxbEhspgUT"
   },
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:47.137182Z",
     "start_time": "2024-03-05T16:49:37.686948Z"
    },
    "id": "NKfM8O-nbbpJ"
   },
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,2), max_features = 30)\n",
    "\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.567610Z",
     "start_time": "2024-03-05T16:49:47.138293Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDCcR3tOpJ-e",
    "outputId": "c1bc4bd9-bf63-4b1a-902b-147b1c243360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.59      0.58       750\n",
      "           2       0.57      0.21      0.31       750\n",
      "           3       0.44      0.80      0.57       750\n",
      "           4       0.69      0.54      0.61       750\n",
      "\n",
      "    accuracy                           0.53      3000\n",
      "   macro avg       0.57      0.53      0.52      3000\n",
      "weighted avg       0.57      0.53      0.52      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - baseline\n",
    "\n",
    "def LR(X_train_tfidf, y_train, X_test_tfidf, y_test):\n",
    "\n",
    "  LR_classifier = LogisticRegression(random_state = 42, max_iter=1000).fit(X_train_tfidf, y_train)\n",
    "\n",
    "  y_pred_lr = LR_classifier.predict(X_test_tfidf)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "  return y_pred_lr\n",
    "\n",
    "y_pred_lr = LR(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621308Z",
     "start_time": "2024-03-05T16:49:53.568257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQyWjGLUxREy",
    "outputId": "e4827d40-70fe-44b0-ecd2-9c271deaf5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.58      0.56       750\n",
      "           2       0.52      0.12      0.20       750\n",
      "           3       0.44      0.85      0.58       750\n",
      "           4       0.63      0.50      0.56       750\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.53      0.51      0.47      3000\n",
      "weighted avg       0.53      0.51      0.47      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB - baseline\n",
    "\n",
    "def NB(X_train_tfidf, y_train, X_test_tfidf, y_test):\n",
    "\n",
    "  nb_classifier = ComplementNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "  y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "  return y_pred_nb\n",
    "\n",
    "y_pred_nb = NB(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITNfwpWkt1OI"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6Yrnd2f_y5W"
   },
   "source": [
    "### Lemmatization with POS Tagging\n",
    "(DON'T RUN AGAIN! It takes tooooo long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:49:53.621497Z",
     "start_time": "2024-03-05T16:49:53.605689Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E3t6cQp1r-N",
    "outputId": "45aad408-e2fc-4eea-d48b-c4cd161199bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/james/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_with_pos(tokens):\n",
    "  # tokenize the sentence and find the POS tag for each token\n",
    "  pos_tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "  # print(pos_tagged)\n",
    "\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "  # print(wordnet_tagged)\n",
    "\n",
    "  lemmatized_sentence = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "      if tag is None:\n",
    "          # if there is no available tag, append the token as is\n",
    "          lemmatized_sentence.append(word)\n",
    "      else:\n",
    "          # else use the tag to lemmatize the token\n",
    "          lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "  # lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "\n",
    "  # print(lemmatized_sentence)\n",
    "  return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:00:54.576560Z",
     "start_time": "2024-03-05T16:49:53.615691Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-3cE9Qzt3zY",
    "outputId": "dd07285c-1ccd-41b5-c9d3-78d8174a7cb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/james/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/james/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 48854/48854 [10:02<00:00, 81.03it/s] \n",
      "100%|██████████| 3000/3000 [00:33<00:00, 88.40it/s] \n"
     ]
    }
   ],
   "source": [
    "# Start preprocessing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def preprocessing(data):\n",
    "  data_clean = []\n",
    "  for sentence in tqdm(data):\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Remove punctuation and number\n",
    "    tokens = [w for w in tokens if (not w in punc) and (not w.isdigit())]\n",
    "\n",
    "    # Spell check\n",
    "    # Taking too long time for each sentence, not practical to be used\n",
    "    # tokens = [spell.correction(w) for w in tokens]\n",
    "    # tokens = [w for w in tokens if w is not None and len(w) > 0]\n",
    "\n",
    "    # Lemmatization based on tagging\n",
    "    tokens = lemmatize_with_pos(tokens)\n",
    "\n",
    "    data_clean.append((' ').join(tokens))\n",
    "  return data_clean\n",
    "\n",
    "X_train_clean = preprocessing(X_train)\n",
    "X_test_clean = preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:47.789779Z",
     "start_time": "2024-03-05T17:04:45.983859Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(X_train_clean, X_test_clean):\n",
    "    with open('raw_data/X_train_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_train_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "    with open('raw_data/X_test_clean.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for doc in X_test_clean:\n",
    "            writer.writerow([doc])\n",
    "\n",
    "# Uncomment if you want to save X_train_clean and X_test_clean\n",
    "# write_to_csv(X_train_clean, X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:04:53.998277Z",
     "start_time": "2024-03-05T17:04:53.082771Z"
    }
   },
   "outputs": [],
   "source": [
    "# def read_from_file():\n",
    "#     a_df = pd.read_csv('raw_data/X_train_clean.csv', header=None, dtype=str)\n",
    "#     a = a_df[0].values\n",
    "    \n",
    "#     b_df = pd.read_csv('raw_data/X_test_clean.csv', header=None, dtype=str)\n",
    "#     b = b_df[0].values\n",
    "#     return a, b\n",
    "\n",
    "# # Uncomment if you want to read from file\n",
    "# a, b = read_from_file()\n",
    "# X_train_clean, X_test_clean = a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:05:11.041172Z",
     "start_time": "2024-03-05T17:04:55.869549Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH-JuFWdzqHa",
    "outputId": "1e641367-5188-4e56-8a83-b04dbde46ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.59      0.56       750\n",
      "           2       0.58      0.16      0.25       750\n",
      "           3       0.46      0.78      0.58       750\n",
      "           4       0.68      0.61      0.64       750\n",
      "\n",
      "    accuracy                           0.54      3000\n",
      "   macro avg       0.56      0.54      0.51      3000\n",
      "weighted avg       0.56      0.54      0.51      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_clean_tfidf, X_test_clean_tfidf = tfidf(X_train_clean, X_test_clean)\n",
    "\n",
    "_ = LR(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:05:12.655783Z",
     "start_time": "2024-03-05T17:05:12.605867Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu2641T-HNd3",
    "outputId": "ba6f10d5-8b1f-4365-be7e-11ca479109bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.59      0.54       750\n",
      "           2       0.54      0.12      0.19       750\n",
      "           3       0.47      0.85      0.60       750\n",
      "           4       0.66      0.53      0.59       750\n",
      "\n",
      "    accuracy                           0.52      3000\n",
      "   macro avg       0.54      0.52      0.48      3000\n",
      "weighted avg       0.54      0.52      0.48      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = NB(X_train_clean_tfidf, y_train, X_test_clean_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords, duplicates, and contradicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:48.406043Z",
     "start_time": "2024-03-05T17:05:14.905065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48652/48652 [01:22<00:00, 588.83it/s] \n",
      "100%|██████████| 2990/2990 [00:04<00:00, 646.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text): #trivial casefolding\n",
    "    return text.lower()\n",
    "\n",
    "#Obtain set of stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    data_removed_stopwords = []\n",
    "    for sentence in tqdm(data):\n",
    "        #tokenize sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        #remove word if found in set of stopwords\n",
    "        words_wo_stopwords = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        data_removed_stopwords.append(' '.join(words_wo_stopwords))\n",
    "    return data_removed_stopwords\n",
    "\n",
    "def remove_dupes(data):\n",
    "    #use pandas to drop duplicates, only removes those with equal values in both class and text columns\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "def remove_contradictions(data):\n",
    "    #group by text and filter groups with more than one unique label(contradicting labels)\n",
    "    contradictions = data.groupby('text').filter(lambda x: x['class'].nunique() > 1)\n",
    "    \n",
    "    #remove all texts with contradicting labels since we do not know which is the correct label\n",
    "    return data[~data['text'].isin(contradictions['text'])]\n",
    "\n",
    "def preprocess2(data):\n",
    "    processed_data = remove_dupes(data)\n",
    "    processed_data = remove_contradictions(processed_data)\n",
    "    processed_data['text'] = remove_stopwords(processed_data['text'])\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "train_clean2 = preprocess2(train)    \n",
    "test_clean2 = preprocess2(test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:54.749892Z",
     "start_time": "2024-03-05T17:06:48.424600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_clean2 = train_clean2['text']\n",
    "y_train_clean2 = train_clean2['class']\n",
    "X_test_clean2 = test_clean2['text']\n",
    "y_test_clean2 = test_clean2['class']\n",
    "\n",
    "X_train_clean2_tfidf, X_test_clean2_tfidf = tfidf(X_train_clean2, X_test_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:58.232005Z",
     "start_time": "2024-03-05T17:06:54.750452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.52      0.51       745\n",
      "           2       0.52      0.08      0.14       750\n",
      "           3       0.38      0.76      0.51       747\n",
      "           4       0.57      0.45      0.50       748\n",
      "\n",
      "    accuracy                           0.45      2990\n",
      "   macro avg       0.49      0.45      0.42      2990\n",
      "weighted avg       0.49      0.45      0.42      2990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = LR(X_train_clean2_tfidf, y_train_clean2, X_test_clean2_tfidf, y_test_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:06:58.286818Z",
     "start_time": "2024-03-05T17:06:58.232950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.55      0.47       745\n",
      "           2       0.41      0.12      0.18       750\n",
      "           3       0.38      0.69      0.49       747\n",
      "           4       0.58      0.33      0.42       748\n",
      "\n",
      "    accuracy                           0.42      2990\n",
      "   macro avg       0.45      0.42      0.39      2990\n",
      "weighted avg       0.45      0.42      0.39      2990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = NB(X_train_clean2_tfidf, y_train_clean2, X_test_clean2_tfidf, y_test_clean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTufYR0Gt4Wi"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:08:08.491871Z",
     "start_time": "2024-03-05T17:06:58.330465Z"
    },
    "id": "h4_ImWMZt6Ru"
   },
   "outputs": [],
   "source": [
    "# Text Subjectivity (w/o preprocessing)\n",
    "from textblob import TextBlob\n",
    "\n",
    "def subjectivity(data, tf_idf):\n",
    "    # new_data = data.copy()\n",
    "    new_data = data\n",
    "    new_data['psycho-linguistic'] = new_data[\"text\"].apply(lambda x: TextBlob(x).sentiment)\n",
    "    new_data['subjectivity'] = new_data['psycho-linguistic'].apply(lambda x: x[1])\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    subject_df = pd.merge(tfidf_df, new_data['subjectivity'],left_index=True, right_index=True)\n",
    "    subject_df.columns = subject_df.columns.astype(str)\n",
    "    return subject_df\n",
    "\n",
    "train_subject_df = subjectivity(train, X_train_tfidf)\n",
    "test_subject_df = subjectivity(test, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:09:11.390660Z",
     "start_time": "2024-03-05T17:08:08.493418Z"
    },
    "id": "eqxPbQkservd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.62      0.62       750\n",
      "           2       0.55      0.19      0.29       750\n",
      "           3       0.44      0.80      0.57       750\n",
      "           4       0.71      0.56      0.63       750\n",
      "\n",
      "    accuracy                           0.55      3000\n",
      "   macro avg       0.58      0.54      0.52      3000\n",
      "weighted avg       0.58      0.55      0.52      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_subject_df, y_train, test_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:09:13.136870Z",
     "start_time": "2024-03-05T17:09:11.435035Z"
    },
    "id": "qLCfOSineuZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.59      0.58       750\n",
      "           2       0.52      0.12      0.19       750\n",
      "           3       0.44      0.85      0.58       750\n",
      "           4       0.64      0.51      0.57       750\n",
      "\n",
      "    accuracy                           0.52      3000\n",
      "   macro avg       0.54      0.52      0.48      3000\n",
      "weighted avg       0.54      0.52      0.48      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_subject_df, y_train, test_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:10:22.321745Z",
     "start_time": "2024-03-05T17:09:13.137714Z"
    },
    "id": "Z8wmIXUvewcx"
   },
   "outputs": [],
   "source": [
    "# Text Subjectivity (with preprocessing)\n",
    "train_clean_subject_df = subjectivity(train, X_train_clean_tfidf)\n",
    "test_clean_subject_df = subjectivity(test, X_test_clean_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:11:14.280845Z",
     "start_time": "2024-03-05T17:10:22.345631Z"
    },
    "id": "Oz5HO9tEey_y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.65      0.60       750\n",
      "           2       0.56      0.15      0.24       750\n",
      "           3       0.46      0.79      0.58       750\n",
      "           4       0.71      0.62      0.66       750\n",
      "\n",
      "    accuracy                           0.55      3000\n",
      "   macro avg       0.57      0.55      0.52      3000\n",
      "weighted avg       0.57      0.55      0.52      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_clean_subject_df, y_train, test_clean_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:11:16.446900Z",
     "start_time": "2024-03-05T17:11:14.310628Z"
    },
    "id": "3Uyp0kzUe1V_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.59      0.55       750\n",
      "           2       0.54      0.12      0.20       750\n",
      "           3       0.47      0.85      0.61       750\n",
      "           4       0.66      0.54      0.59       750\n",
      "\n",
      "    accuracy                           0.53      3000\n",
      "   macro avg       0.55      0.53      0.49      3000\n",
      "weighted avg       0.55      0.53      0.49      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_clean_subject_df, y_train, test_clean_subject_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "We will adjust the possible parameters, i.e. n_grams, max_features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m X_train_tfidf, X_test_tfidf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_gram \u001b[38;5;129;01min\u001b[39;00m n_grams:\n\u001b[0;32m---> 12\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m max_feature \u001b[38;5;129;01min\u001b[39;00m max_features:\n\u001b[1;32m     13\u001b[0m     X_train_tfidf, X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf(X_train_clean, X_test_clean, n_gram, max_feature)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_gram: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_gram\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_feature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_feature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# All this need Kfold cross validation in the future\n",
    "\n",
    "n_grams = [1, 2] # 3, 4]\n",
    "max_features = 29 #[10000, 20000, None]\n",
    "def tfidf(X_train, X_test, n_gram, max_feature):\n",
    "  tfidf_baseline = TfidfVectorizer(ngram_range=(1,n_gram), max_features = max_feature)\n",
    "  X_train_tfidf = tfidf_baseline.fit_transform(X_train)\n",
    "  X_test_tfidf = tfidf_baseline.transform(X_test)\n",
    "  return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "for n_gram in n_grams:\n",
    "  for max_feature in max_features:\n",
    "    X_train_tfidf, X_test_tfidf = tfidf(X_train_clean, X_test_clean, n_gram, max_feature)\n",
    "    print(f\"n_gram: {n_gram}, max_feature: {max_feature}\")\n",
    "    num_features = X_train_tfidf.shape[1]\n",
    "    if max_feature is None:\n",
    "      print(f\"Number of features: {num_features}\")\n",
    "    _ = LR(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "    _ = NB(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "\n",
    "# Ran until n_gram = 3 for 30min ><"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "#### GloVe\n",
    "- Download from https://nlp.stanford.edu/projects/glove/\n",
    "- Used Wikipedia 2014 + Gigaword 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVec = KeyedVectors.load(\"word2vec-google-news-300.model\", mmap='r')\n",
    "def meanEmbed(text, wordVec):\n",
    "    mean = []\n",
    "    \n",
    "    for t in text:\n",
    "        wordVecs = []\n",
    "\n",
    "        for word in t.split():\n",
    "            if word in wordVec:\n",
    "                wordVecs.append(wordVec[word])\n",
    "    \n",
    "        if not wordVecs:\n",
    "            wordVecs.append(np.zeros(wordVec.vector_size))\n",
    "\n",
    "        tempmean = np.mean(wordVecs, axis=0)\n",
    "        mean.append(tempmean)\n",
    "\n",
    "    return np.array(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m             glove_model[word] \u001b[38;5;241m=\u001b[39m coefs\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m glove_model\n\u001b[0;32m---> 15\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove.6B/glove.6B.300d.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m, in \u001b[0;36mload_glove_model\u001b[0;34m(glove_file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_glove_model\u001b[39m(glove_file_path):\n\u001b[1;32m      2\u001b[0m     glove_model \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m             split_line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/Documents/NUS/Y2S2/CS4248/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    glove_model = {}\n",
    "    with open(glove_file_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            # embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            try:\n",
    "                coefs = np.asarray(split_line[1:], dtype='float32')\n",
    "            except ValueError:\n",
    "                pass\n",
    "            glove_model[word] = coefs\n",
    "    return glove_model\n",
    "\n",
    "# glove_model = load_glove_model(\"glove.6B/glove.6B.300d.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.33712   -0.21691   -0.0066365 -0.41625   -1.2555   ]\n"
     ]
    }
   ],
   "source": [
    "# print(glove_model['hello'].shape)\n",
    "# print(glove_model['hello'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_features(tokens, glove_model):\n",
    "#     vectors = [glove_model[token] for token in tokens if token in glove_model]\n",
    "#     if vectors:\n",
    "#         return np.mean(vectors, axis=0) # Return the mean of the vectors\n",
    "#     else:\n",
    "#         return np.zeros(300)  # Return a zero vector if no words are found\n",
    "\n",
    "# # Convert X_train_clean to a pandas Series\n",
    "# X_train_clean_series = pd.Series(X_train_clean)\n",
    "# # Convert text to features using a predefined function (e.g., using GloVe embeddings)\n",
    "# glove_embedding_features = X_train_clean_series.apply(lambda x: pd.Series(text_to_features(x, glove_model)))\n",
    "# glove_embedding_features.columns = [f'Feature_{i}' for i in range(300)]\n",
    "\n",
    "# # Use the same function to convert X_test_clean to a pandas Series\n",
    "# X_test_clean_series = pd.Series(X_test_clean)\n",
    "# glove_embedding_features_test = X_test_clean_series.apply(lambda x: pd.Series(text_to_features(x, glove_model)))\n",
    "# glove_embedding_features_test.columns = [f'Feature_{i}' for i in range(300)]\n",
    "\n",
    "# # Runs for a short while : ~45s on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0  -0.246974   0.048534  -0.288494  -0.078190  -0.596826  -0.001970   \n",
      "1  -0.237516   0.038033  -0.291854  -0.056598  -0.580772  -0.006075   \n",
      "2  -0.256160   0.036380  -0.281820  -0.076163  -0.587690   0.000572   \n",
      "3  -0.244585   0.044431  -0.285958  -0.077551  -0.579946  -0.011869   \n",
      "4  -0.262932   0.050881  -0.302546  -0.065594  -0.586354  -0.021741   \n",
      "\n",
      "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_290  Feature_291  \\\n",
      "0  -0.074773   0.236836  -0.342276  -1.160948  ...    -0.159519     0.036412   \n",
      "1  -0.077081   0.216868  -0.343079  -1.178258  ...    -0.152523     0.026440   \n",
      "2  -0.061376   0.217344  -0.324343  -1.165821  ...    -0.151057     0.010553   \n",
      "3  -0.065669   0.223504  -0.321919  -1.182414  ...    -0.152639     0.010548   \n",
      "4  -0.074013   0.224716  -0.313182  -1.172550  ...    -0.161936     0.016166   \n",
      "\n",
      "   Feature_292  Feature_293  Feature_294  Feature_295  Feature_296  \\\n",
      "0    -0.198667    -0.091196     0.138900     0.135555     0.545077   \n",
      "1    -0.191758    -0.111663     0.119745     0.092159     0.534727   \n",
      "2    -0.187380    -0.103910     0.124305     0.109556     0.513506   \n",
      "3    -0.195501    -0.120852     0.129306     0.096141     0.524307   \n",
      "4    -0.192070    -0.127031     0.103602     0.096330     0.519451   \n",
      "\n",
      "   Feature_297  Feature_298  Feature_299  \n",
      "0    -0.569282    -0.112915     0.126879  \n",
      "1    -0.568758    -0.126068     0.146734  \n",
      "2    -0.557550    -0.123620     0.116890  \n",
      "3    -0.550046    -0.102473     0.132888  \n",
      "4    -0.553605    -0.097683     0.112134  \n",
      "\n",
      "[5 rows x 300 columns]\n",
      "(48854, 300)\n"
     ]
    }
   ],
   "source": [
    "# print(glove_embedding_features.head())\n",
    "# print(glove_embedding_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.23      0.22       750\n",
      "           2       0.57      0.22      0.32       750\n",
      "           3       0.19      0.33      0.24       750\n",
      "           4       0.66      0.53      0.59       750\n",
      "\n",
      "    accuracy                           0.33      3000\n",
      "   macro avg       0.41      0.33      0.34      3000\n",
      "weighted avg       0.41      0.33      0.34      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.24      0.28      0.26       750\n",
      "           2       0.37      0.85      0.52       750\n",
      "           3       0.03      0.01      0.01       750\n",
      "           4       0.65      0.25      0.36       750\n",
      "\n",
      "    accuracy                           0.35      3000\n",
      "   macro avg       0.32      0.35      0.29      3000\n",
      "weighted avg       0.32      0.35      0.29      3000\n"
     ]
    }
   ],
   "source": [
    "# # Cleaned data + GloVe embeddings + Logistic Regression\n",
    "# _ = LR(glove_embedding_features, y_train, glove_embedding_features_test, y_test)\n",
    "\n",
    "# # Cleaned data + GloVe embeddings + Naive Bayes\n",
    "# # _ = NB(glove_embedding_features, y_train, glove_embedding_features_test, y_test)\n",
    "# # ValueError: Negative values in data passed to ComplementNB (input X)\n",
    "\n",
    "# # Cleaned data + GloVe embeddings + Naive Bayes (as a workaround)\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# classifier = GaussianNB()\n",
    "# _ = classifier.fit(glove_embedding_features.to_numpy(), y_train)\n",
    "# y_pred = classifier.predict(glove_embedding_features_test.to_numpy())\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:28:24.733933Z",
     "start_time": "2024-03-05T17:27:58.208190Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hedged_df\n\u001b[1;32m     29\u001b[0m hedges_re \u001b[38;5;241m=\u001b[39m fetch_hedging_regex()\n\u001b[0;32m---> 30\u001b[0m train_hedge_df \u001b[38;5;241m=\u001b[39m hedging_word_count(\u001b[43mtrain\u001b[49m, X_train_tfidf, hedges_re)\n\u001b[1;32m     31\u001b[0m test_hedge_df \u001b[38;5;241m=\u001b[39m hedging_word_count(test, X_test_tfidf, hedges_re)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Hedging Word Count\n",
    "# Reference: all 176 hedging words in hedges.txt are from https://github.com/words/hedges\n",
    "import re\n",
    "\n",
    "def fetch_hedging_regex(filename='hedges.txt'):\n",
    "    hedges = []\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    for l in lines:\n",
    "        hedges.append(l.strip())\n",
    "    hedges_re = \"(?:^|\\W)(\" + \"|\".join(hedges) + \")(?:$|\\W)\"\n",
    "    return hedges_re\n",
    "\n",
    "def hedges(x, hedges_re):\n",
    "    res = re.findall(hedges_re, x.lower())\n",
    "    counts = sum(1 for s in res if s != '')\n",
    "    if counts > 0:\n",
    "        return 1 \n",
    "    return 0\n",
    "\n",
    "def hedging_word_count(data, tf_idf, hedge_re):\n",
    "    new_data = data.copy()\n",
    "    new_data[\"hedges\"] = new_data.loc[:, \"text\"].apply(hedges, hedges_re=hedge_re)\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    hedged_df = pd.merge(tfidf_df, new_data['hedges'],left_index=True, right_index=True)\n",
    "    hedged_df.columns = hedged_df.columns.astype(str)\n",
    "    return hedged_df\n",
    "\n",
    "hedges_re = fetch_hedging_regex()\n",
    "train_hedge_df = hedging_word_count(train, X_train_tfidf, hedges_re)\n",
    "test_hedge_df = hedging_word_count(test, X_test_tfidf, hedges_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:29:12.802388Z",
     "start_time": "2024-03-05T17:28:24.756603Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mLR\u001b[49m(train_hedge_df, y_train, test_hedge_df, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LR' is not defined"
     ]
    }
   ],
   "source": [
    "_ = LR(train_hedge_df, y_train, test_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:29:15.025973Z",
     "start_time": "2024-03-05T17:29:12.842389Z"
    },
    "id": "V_EZcIXQfD-t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.62      0.62       750\n",
      "           2       0.62      0.46      0.53       750\n",
      "           3       0.62      0.93      0.74       750\n",
      "           4       0.88      0.67      0.76       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.66      3000\n",
      "weighted avg       0.69      0.67      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_hedge_df, y_train, test_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:29:40.772374Z",
     "start_time": "2024-03-05T17:29:15.066556Z"
    },
    "id": "rYz1yM-ffFyo"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hedging Word Count (with preprocessing)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_clean_hedge_df \u001b[38;5;241m=\u001b[39m hedging_word_count(\u001b[43mtrain\u001b[49m, X_train_clean_tfidf, hedges_re)\n\u001b[1;32m      3\u001b[0m test_clean_hedge_df \u001b[38;5;241m=\u001b[39m hedging_word_count(test, X_test_clean_tfidf, hedges_re)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Hedging Word Count (with preprocessing)\n",
    "train_clean_hedge_df = hedging_word_count(train, X_train_clean_tfidf, hedges_re)\n",
    "test_clean_hedge_df = hedging_word_count(test, X_test_clean_tfidf, hedges_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:30:46.644960Z",
     "start_time": "2024-03-05T17:29:40.795476Z"
    },
    "id": "mYjo7t8AfIMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.83      0.84       750\n",
      "           2       0.83      0.41      0.54       750\n",
      "           3       0.59      0.82      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.74      0.73      3000\n",
      "weighted avg       0.77      0.74      0.73      3000\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_clean_hedge_df, y_train, test_clean_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T17:30:48.552354Z",
     "start_time": "2024-03-05T17:30:46.675254Z"
    },
    "id": "GzySa4unfL_F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.61      0.61       750\n",
      "           2       0.64      0.51      0.57       750\n",
      "           3       0.63      0.91      0.75       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.67      3000\n",
      "   macro avg       0.69      0.67      0.67      3000\n",
      "weighted avg       0.69      0.67      0.67      3000\n"
     ]
    }
   ],
   "source": [
    "_ = NB(train_clean_hedge_df, y_train, test_clean_hedge_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T13:38:37.122924Z",
     "start_time": "2024-03-03T13:38:37.121971Z"
    },
    "id": "HgSb6hNwfNmO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_features(data, tf_idf):\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    new_data['sentiments'] = new_data['text'].apply(lambda x: sia.polarity_scores(x))\n",
    "    new_data['compound'] = new_data['sentiments'].apply(lambda x: x['compound'])\n",
    "    tfidf_df = pd.DataFrame(tf_idf.toarray())\n",
    "    sentiment_df = pd.merge(tfidf_df, new_data['compound'], left_index=True, right_index=True)\n",
    "    sentiment_df.columns = sentiment_df.columns.astype(str)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "train_sentiment_df = sentiment_features(train, X_train_tfidf)\n",
    "test_sentiment_df = sentiment_features(test, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.80      0.82       750\n",
      "           2       0.82      0.37      0.52       750\n",
      "           3       0.56      0.83      0.67       750\n",
      "           4       0.81      0.92      0.86       750\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.76      0.73      0.72      3000\n",
      "weighted avg       0.76      0.73      0.72      3000\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_sentiment_df, y_train, test_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.64      0.62       750\n",
      "           2       0.62      0.45      0.52       750\n",
      "           3       0.62      0.92      0.74       750\n",
      "           4       0.88      0.66      0.75       750\n",
      "\n",
      "    accuracy                           0.66      3000\n",
      "   macro avg       0.68      0.66      0.66      3000\n",
      "weighted avg       0.68      0.66      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Scores may be negative so NB will not work. For NB, I have added a constant to the sentiment scores to make them all positive.\n",
    "min_compound_train = train_sentiment_df['compound'].min()\n",
    "min_compound_test = test_sentiment_df['compound'].min()\n",
    "min_compound = min(min_compound_train, min_compound_test)\n",
    "\n",
    "if min_compound < 0:\n",
    "    adjustment = abs(min_compound)\n",
    "    train_sentiment_df['compound'] += adjustment\n",
    "    test_sentiment_df['compound'] += adjustment\n",
    "\n",
    "_ = NB(train_sentiment_df, y_train, test_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Preprocessing\n",
    "train_clean_sentiment_df = sentiment_features(train, X_train_clean_tfidf)\n",
    "test_clean_sentiment_df = sentiment_features(test, X_test_clean_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.81      0.82       750\n",
      "           2       0.84      0.42      0.56       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.74      0.73      3000\n",
      "weighted avg       0.77      0.74      0.73      3000\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_clean_sentiment_df, y_train, test_clean_sentiment_df y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.62      0.61       750\n",
      "           2       0.63      0.48      0.54       750\n",
      "           3       0.62      0.91      0.74       750\n",
      "           4       0.87      0.64      0.74       750\n",
      "\n",
      "    accuracy                           0.66      3000\n",
      "   macro avg       0.68      0.66      0.66      3000\n",
      "weighted avg       0.68      0.66      0.66      3000\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Scores may be negative so NB will not work. For NB, I have added a constant to the sentiment scores to make them all positive.\n",
    "min_clean_compound_train = train_clean_sentiment_df['compound'].min()\n",
    "min_clean_compound_test = test_clean_sentiment_df['compound'].min()\n",
    "min_clean_compound = min(min_clean_compound_train, min_clean_compound_test)\n",
    "\n",
    "if min_clean_compound < 0:\n",
    "    adjustment = abs(min_clean_compound)\n",
    "    train_clean_sentiment_df['compound'] += adjustment\n",
    "    test_clean_sentiment_df['compound'] += adjustment\n",
    "    \n",
    "_ = NB(train_clean_sentiment_df, y_train, test_clean_sentiment_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Length\n",
    "- Number of quotations\n",
    "- Count of word types:\n",
    "    - Numbers\n",
    "    - Proper nouns\n",
    "    - Conjunctions\n",
    "    - Superlatives\n",
    "    - 1st / 3rd pronoun counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhizhouhuang/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.84      0.85       750\n",
      "           2       0.84      0.44      0.58       750\n",
      "           3       0.60      0.82      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.75      3000\n",
      "   macro avg       0.78      0.75      0.74      3000\n",
      "weighted avg       0.78      0.75      0.74      3000\n",
      "\n",
      "Time taken for Sentence Length on Not Processed dataset: 93.36047506332397 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.79      0.82       750\n",
      "           2       0.81      0.37      0.51       750\n",
      "           3       0.56      0.83      0.67       750\n",
      "           4       0.81      0.92      0.86       750\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.76      0.73      0.71      3000\n",
      "weighted avg       0.76      0.73      0.71      3000\n",
      "\n",
      "Time taken for Count Quotations on Not Processed dataset: 7.237422943115234 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.82      0.84       750\n",
      "           2       0.83      0.44      0.57       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.75      3000\n",
      "   macro avg       0.77      0.75      0.74      3000\n",
      "weighted avg       0.77      0.75      0.74      3000\n",
      "\n",
      "Time taken for Count Numbers on Not Processed dataset: 57.37210702896118 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhizhouhuang/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.81      0.83       750\n",
      "           2       0.84      0.43      0.56       750\n",
      "           3       0.58      0.80      0.68       750\n",
      "           4       0.79      0.91      0.85       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.76      0.74      0.73      3000\n",
      "weighted avg       0.76      0.74      0.73      3000\n",
      "\n",
      "Time taken for Count Proper Nouns on Not Processed dataset: 672.8190791606903 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.81      0.83       750\n",
      "           2       0.82      0.42      0.56       750\n",
      "           3       0.59      0.83      0.69       750\n",
      "           4       0.81      0.91      0.86       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.77      0.74      0.74      3000\n",
      "weighted avg       0.77      0.74      0.74      3000\n",
      "\n",
      "Time taken for Count Conjunctions on Not Processed dataset: 667.076840877533 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot Processed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     X_train_enhanced \u001b[38;5;241m=\u001b[39m \u001b[43madd_custom_feature_to_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     X_test_enhanced \u001b[38;5;241m=\u001b[39m add_custom_feature_to_tfidf(X_test, X_test_tfidf, feature_matrix)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36madd_custom_feature_to_tfidf\u001b[0;34m(data, tfidf_matrix, feature_extraction_func)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_custom_feature_to_tfidf\u001b[39m(data, tfidf_matrix, feature_extraction_func):\n\u001b[0;32m---> 18\u001b[0m     custom_feature \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_extraction_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     custom_feature_sparse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(custom_feature)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     enhanced_tfidf_matrix \u001b[38;5;241m=\u001b[39m hstack([tfidf_matrix, custom_feature_sparse])\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mcount_superlatives\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_superlatives\u001b[39m(text):\n\u001b[0;32m---> 44\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     superlatives \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(superlatives)\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/nltk/tag/perceptron.py:186\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    182\u001b[0m tag, conf \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdict\u001b[38;5;241m.\u001b[39mget(word), \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_tagdict \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[0;32m--> 186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(features, return_conf)\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n",
      "File \u001b[0;32m~/Desktop/Labeled-Unreliable-News/venv/lib/python3.10/site-packages/nltk/tag/perceptron.py:295\u001b[0m, in \u001b[0;36mPerceptronTagger._get_features\u001b[0;34m(self, i, word, context, prev, prev2)\u001b[0m\n\u001b[1;32m    293\u001b[0m features \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# It's useful to have a constant feature, which acts sort of like a prior\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m add(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:])\n\u001b[1;32m    297\u001b[0m add(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi pref1\u001b[39m\u001b[38;5;124m\"\u001b[39m, word[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Again, all this need Kfold cross validation in the future\n",
    "\n",
    "# Count sentence length\n",
    "# Count number of quotations\n",
    "# Count of word types: Numbers, Proper nouns, Conjunctions, Superlatives, 1st / 3rd pronoun counts (w/o preprocessing)\n",
    "# Count of booster words (with preprocessing)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "\n",
    "# Uncomment if you have not downloaded the 'punkt' and 'averaged_perceptron_tagger' packages\n",
    "# nltk.download('punkt') \n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def add_custom_feature_to_tfidf(data, tfidf_matrix, feature_extraction_func):\n",
    "    custom_feature = data.apply(feature_extraction_func)\n",
    "    custom_feature_sparse = np.array(custom_feature).reshape(-1, 1)\n",
    "    enhanced_tfidf_matrix = hstack([tfidf_matrix, custom_feature_sparse])\n",
    "    return enhanced_tfidf_matrix\n",
    "\n",
    "# Feature extraction functions\n",
    "def count_sentence_length(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_quotations(text):\n",
    "    return text.count('\"')\n",
    "\n",
    "def count_numbers(text):\n",
    "    return len(re.findall(r'\\d+', text))\n",
    "\n",
    "def count_proper_nouns(text):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    proper_nouns = [word for word, pos in tagged if pos == 'NNP']\n",
    "    return len(proper_nouns)\n",
    "\n",
    "def count_conjunctions(text):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    conjunctions = [word for word, pos in tagged if pos == 'CC']\n",
    "    return len(conjunctions)\n",
    "\n",
    "def count_superlatives(text):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    superlatives = [word for word, pos in tagged if pos == 'JJS']\n",
    "    return len(superlatives)\n",
    "\n",
    "def count_1st_pronouns(text):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    first_pronouns = [word for word, pos in tagged if pos == 'PRP' and word.lower() in ['i', 'me', 'my', 'mine']]\n",
    "    return len(first_pronouns)\n",
    "\n",
    "def count_3rd_pronouns(text):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    third_pronouns = [word for word, pos in tagged if pos == 'PRP' and word.lower() in \n",
    "                      ['he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']]\n",
    "    return len(third_pronouns)\n",
    "\n",
    "def count_booster_words(text):\n",
    "    booster_words = \"\"\"absolutely amazingly awfully completely considerably definitely entirely exceedingly\n",
    "                    extremely highly incredibly indeed quite really remarkably so terribly totally truly utterly very\"\"\"\n",
    "    booster_words = booster_words.split(\" \")\n",
    "    return len([word for word in text.split(\" \") if word in booster_words])\n",
    "\n",
    "# Evaluate the performance of the model with the custom features\n",
    "# For the purpose of comparison, we can just use the logistic regression model across all the custom features\n",
    "features_methods_dictionary =  {\n",
    "    \"Sentence Length\": count_sentence_length,\n",
    "    \"Count Quotations\": count_quotations,\n",
    "    \"Count Numbers\": count_numbers,\n",
    "    \"Count Proper Nouns\": count_proper_nouns,\n",
    "    \"Count Conjunctions\": count_conjunctions,\n",
    "    \"Count Superlatives\": count_superlatives,\n",
    "    \"Count 1st Pronouns\": count_1st_pronouns,\n",
    "    \"Count 3rd Pronouns\": count_3rd_pronouns,\n",
    "    \"Count Booster Words\": count_booster_words\n",
    "}\n",
    "\n",
    "training_datasets = {\n",
    "    \"Not Processed\": [X_train, X_train_tfidf, X_test, X_test_tfidf],\n",
    "    \"Processed\": [X_train_clean, X_train_clean_tfidf, X_test_clean, X_test_clean_tfidf]\n",
    "}\n",
    "for dataset_name , datasets in training_datasets.items():\n",
    "    X_train = datasets[0]\n",
    "    X_train_tfidf = datasets[1]\n",
    "    X_test = datasets[2]\n",
    "    X_test_tfidf = datasets[3]\n",
    "    for feature_name, feature_matrix in features_methods_dictionary.items():\n",
    "        time_start = time.time()\n",
    "        if dataset_name == \"Not Processed\":\n",
    "            X_train_enhanced = add_custom_feature_to_tfidf(X_train, X_train_tfidf, feature_matrix)\n",
    "            X_test_enhanced = add_custom_feature_to_tfidf(X_test, X_test_tfidf, feature_matrix)\n",
    "        else:\n",
    "            X_train_enhanced = add_custom_feature_to_tfidf(X_train, X_train_tfidf, feature_matrix)\n",
    "            X_test_enhanced = add_custom_feature_to_tfidf(X_test, X_test_tfidf, feature_matrix)\n",
    "        _ = LR(X_train_enhanced, y_train, X_test_enhanced, y_test)\n",
    "        time_end = time.time()\n",
    "        print(f\"Time taken for {feature_name} on {dataset_name} dataset: {time_end - time_start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(train_df, test_df):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df)\n",
    "    test_scaled = scaler.transform(test_df)\n",
    "    \n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns)\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns)\n",
    "    \n",
    "    return train_scaled_df, test_scaled_df\n",
    "\n",
    "train_length_scaled_df, test_length_scaled_df = scale_features(X_train_enhanced, X_test_enhanced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.74      0.79       750\n",
      "           2       0.73      0.40      0.51       750\n",
      "           3       0.57      0.73      0.64       750\n",
      "           4       0.74      0.94      0.82       750\n",
      "\n",
      "    accuracy                           0.70      3000\n",
      "   macro avg       0.72      0.70      0.69      3000\n",
      "weighted avg       0.72      0.70      0.69      3000\n"
     ]
    }
   ],
   "source": [
    "_ = LR(train_length_scaled_df, y_train, test_length_scaled_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.86      0.74       750\n",
      "           2       0.72      0.42      0.53       750\n",
      "           3       0.56      0.93      0.70       750\n",
      "           4       0.90      0.39      0.55       750\n",
      "\n",
      "    accuracy                           0.65      3000\n",
      "   macro avg       0.71      0.65      0.63      3000\n",
      "weighted avg       0.71      0.65      0.63      3000\n"
     ]
    }
   ],
   "source": [
    "# For some reason, after scaling sentence length can give negative values so I will just use NB with unscaled sentence length.\n",
    "\n",
    "_ = NB(X_train_enhanced, y_train, X_test_enhanced, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
